{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from utils import *\n",
    "from YourAnswer import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed\n",
    "- Fix the seed to constraint the randomness and reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use the CIFAR10 dataset. <br>\n",
    "- Load the data as numpy type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images\n",
    "- Check what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*10000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `softmax` in `YourAnswer.py` file <br> \n",
    "\n",
    "<center> $\\Large p(\\mathbf{y}=i|\\mathbf{x}; { {\\theta}_{1} , ... , {\\theta}_{k} } )$ =  $\\Large \\frac{exp({\\theta}_{i}^T) \\mathbf{x}} {\\Sigma_{j=1}^{k} exp({\\theta}_{j}^T) \\mathbf{x}}$  <br><br>\n",
    "    where $\\mathbf{x}\\in \\mathbb{R}^{d}$ and $\\mathbf{y}\\in\\mathbb{R}^{k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be [2.06115362e-09 1.80485138e-35 9.99999998e-01]\n",
    "- Sum of the softmax output should be 0.9999999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080]])\n",
    "softmax_result1 = softmax(temp_x)\n",
    "print('Softmax result :\\n',softmax_result1)\n",
    "print ('\\nSum of the softmax :',np.sum(softmax_result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be <br>\n",
    "    [2.06115362e-09   1.80485138e-35   9.99999998e-01] <br>\n",
    "    [2.06106005e-09   4.53978686e-05   9.99954600e-01]]\n",
    " \n",
    "- Sum of the softmax output should be [1. 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080],[1010,1020,1030]])\n",
    "softmax_result2 = softmax(temp_x)\n",
    "print('Softmax result :\\n',softmax_result2)\n",
    "print ('\\nSum of the softmax :',np.sum(softmax_result2,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `cross_entropy_loss` in `YourAnswer.py` file <br> \n",
    "- Here, we will use `temp_score` instead of $h_{\\theta}(x)$\n",
    "- Loss function is composed of data loss and regularization loss\n",
    "<center> $ L(\\theta)$ = $ \\frac{1}{N} \\Sigma_{i=1}^{N} L_{i}{(h_{\\theta},(x_{i},y_{i}))} + \\lambda R(\\theta)$ <br><br>\n",
    "\n",
    "- We choose to use cross entropy loss <br> \n",
    "   <center>$L_{i} = - y_{i} log(h_{\\theta}(x_{i}))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_theta1 = np.array([[0.1,0.2,0.3],[-0.5,0.3,-0.8]])\n",
    "temp_theta2 = np.array([[0.9,-0.5,0.3],[0.9,0.6,-0.8]])\n",
    "\n",
    "thetas = {}\n",
    "thetas['T1'] = (temp_theta1)\n",
    "thetas['T2'] = (temp_theta2)\n",
    "\n",
    "reg_term = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 20.72530583694641\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score0 = np.array([[0.0, 0.0, 0.0]])\n",
    "temp_target0 = np.array([[0,1,0]])\n",
    "loss0 = cross_entropy_loss(temp_score0, temp_target0, thetas, reg_term)\n",
    "print('Total Loss for temp_0 =', loss0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 1.2060128009926026\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score1 = np.array([[0.1, 0.3, 0.6]])\n",
    "temp_target1 = np.array([[0,1,0]])\n",
    "loss1 = cross_entropy_loss(temp_score1, temp_target1 , thetas, reg_term)\n",
    "print('Total Loss for temp_1 =', loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 0.7439146816378243\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score2 = np.array([[0.1, 0.3, 0.6],[0.2,0.4,0.4],[0.9,0.05,0.05]])\n",
    "temp_target2 = np.array([[0,1,0],[0,0,1],[1,0,0]])\n",
    "loss2 = cross_entropy_loss(temp_score2, temp_target2 , thetas, 0.001)\n",
    "print('Total Loss for temp_2 =', loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `OutputLayer` in `YourAnswer.py` file \n",
    "- Output layer is a layer where the softmax score and loss are computed\n",
    "- Use Cross Entropy Loss this time\n",
    "- W in the picture below is the same as $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('fig/Output_Layer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer = OutputLayer(thetas, reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward output should be 13.097100867144416\n",
    "- Backward output should be [ 0.90887517, -0.99999795,  0.09112277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_t1 = np.array([[0,1,0]])\n",
    "output_forward1 = outputlayer.forward(temp_x1, temp_t1)\n",
    "output_backward1 = outputlayer.backward()\n",
    "print('Forward propagation of output layer :', output_forward1)\n",
    "print('Backward propagation of output layer :', output_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward output should be 7.077588386844261\n",
    "- Backward output should be [ 3.02958391e-01, -3.33332649e-01,  3.03742579e-02],\n",
    "       [-3.32509126e-01,  3.32509088e-01,  3.74189683e-08],\n",
    "       [ 7.26173786e-04,  2.92959414e-01, -2.93685588e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_t2 = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "output_forward2 = outputlayer.forward(temp_x2, temp_t2)\n",
    "output_backward2 = outputlayer.backward()\n",
    "print('Forward propagation of output layer :', output_forward2)\n",
    "print('\\nBackward propagation of output layer :', output_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `ReLU` in `YourAnswer.py` file <br><br> \n",
    "- ReLU passes only the positive values and sets the non-positive values as zero\n",
    "- We can mathematically describe it as follows :\n",
    "<center> $\\Large ReLU(x) =  max(0,x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation should be [3.  0.  0.7]\n",
    "- Backward propagation should be [-10   0   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_x2 = np.array([[-10,1,0]])\n",
    "relu_forward1 = relu.forward(temp_x1)\n",
    "relu_backward1 = relu.backward(temp_x2)\n",
    "print('Forward propagation of ReLU :', relu_forward1)\n",
    "print('Backward propagation of ReLU :', relu_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation should be \n",
    "            [ 3. ,  0. ,  0.7],\n",
    "            [ 9. , 15. ,  0. ],\n",
    "            [ 0. ,  1. ,  0. ]\n",
    "<br>\n",
    "- Backward propagation should be\n",
    "            [  3,   0, -10],\n",
    "            [  5,  -4,   0],\n",
    "            [  0,  -5,   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x3 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_x4 = np.array([[3,5,-10],[5,-4,2],[-3,-5,3]])\n",
    "relu_forward2 = relu.forward(temp_x3)\n",
    "relu_backward2 = relu.backward(temp_x4)\n",
    "print('Forward propagation of ReLU :', relu_forward2)\n",
    "print('\\nBackward propagation of ReLU :', relu_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `ReLU` in `YourAnswer.py` file <br><br> \n",
    "- Sigmoid is an activation function which converts all the values between 0 and 1 \n",
    "- The mathematical description is as follows\n",
    "\n",
    "<center> $\\Large  \\sigma(x) =  \\frac{1}{1+exp^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be [9.52574127e-01 4.53978687e-05 6.68187772e-01]\n",
    "- Backward propagation output should be [ 0.13552998 -0.00045396  0.15519901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "sigmoid_forward1 = sigmoid.forward(temp_x1)\n",
    "sigmoid_backward1 = sigmoid.backward(temp_x1)\n",
    "print('Forward propagation of sigmoid :',sigmoid_forward1)\n",
    "print('Backward propagation of sigmoid :',sigmoid_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be <br>\n",
    "   [9.52574127e-01 4.53978687e-05 6.68187772e-01], <br>\n",
    "   [9.99876605e-01 9.99999694e-01 2.68941421e-01], <br>\n",
    "   [6.69285092e-03 7.31058579e-01 2.68941421e-01]\n",
    "\n",
    "- Backward propagation output should be  <br>\n",
    " [ 1.35529979e-01 -4.53958077e-04  1.55199011e-01], <br>\n",
    " [ 1.11041415e-03  4.58853200e-06 -1.96611933e-01], <br>\n",
    " [-3.32402834e-02  1.96611933e-01 -1.96611933e-01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "sigmoid_forward2 = sigmoid.forward(temp_x2)\n",
    "sigmoid_backward2 = sigmoid.backward(temp_x2)\n",
    "print('Forward propagation of sigmoid :',sigmoid_forward2)\n",
    "print('\\nBackward propagation of sigmoid :',sigmoid_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `Affine` in `YourAnswer.py` file <br><br> \n",
    "- Affine layer connects the input and output with weights and bias\n",
    "- It is also called as dense layer or linear layer\n",
    "\n",
    "<center>   Affine$(\\theta,b) =\\Large \\theta X + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be <br>\n",
    "[ 0.51 -0.39  0.84] <br>\n",
    " [-0.07 -0.02  0.02]\n",
    " \n",
    "- Backward propagation output should be <br>\n",
    "[-0.61  0.28] <br>\n",
    " [-0.25 -0.21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_theta = np.array([[0.2, -0.3, 0.6], [-0.9, 0.1, -0.4]])\n",
    "temp_bias = np.array([[0.2, -0.3, 0.6]])\n",
    "temp_x = np.array([[0.2, -0.3], [-0.9, 0.1]])\n",
    "temp_gradient = np.array([[0.1, 0.5, -0.8], [0.4, 0.7, -0.2]])\n",
    "\n",
    "affine = Affine(temp_theta, temp_bias)\n",
    "affine_forward1 = affine.forward(temp_x)\n",
    "affine_backward1 = affine.backward(temp_gradient)\n",
    "print('Forward propagation of Affine :\\n', affine_forward1)\n",
    "print('\\nBackward propagation of Affine :\\n', affine_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dW of affine should be <br>\n",
    "        [-0.34, -0.53,  0.02],\n",
    "        [ 0.01, -0.08,  0.22]\n",
    "<br>\n",
    "- db of affine should be\n",
    "        [ 0.5,  1.2, -1. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = affine.dT\n",
    "db = affine.db\n",
    "print('Gradient of the thetas :\\n', dt)\n",
    "print('\\nGradient of the biases :',db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. TwoLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `TwoLayerNet` in `YourAnswer.py` file <br><br> \n",
    "- Construct a two layer NN\n",
    "- You need to implement both the forward pass and the backward pass\n",
    "- Use the functions you implemented so far\n",
    "- You can find some helps about the OrderedDict here (https://pymotw.com/2/collections/ordereddict.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical gradient vs Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=3072, hidden_size=10, output_size=10, regularization = 100)\n",
    "\n",
    "x_batch = X_tr[:3]\n",
    "t_batch = Y_tr[:3]\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "targets = t_batch.reshape(-1)\n",
    "t_batch = np.eye(nb_classes)[targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running time of grad_backprop should be much faster than the time of grad_numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "print(\"[grad_backprop] running time(sec) : \" +str(time.time() - start_time))\n",
    "\n",
    "start_time = time.time() \n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "print(\"[grad_numerical] running time(sec) : \"+str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Cifar-10 dataset Label as One-hot vector\n",
    "- The labels were the natural language like deer, horse, car\n",
    "- We need to change it to One-hot vector so that the machine can differentiate them\n",
    "- One-hot vector is a vector that only one element is 1 and the others are 0. <br>\n",
    "\n",
    "  For example : \n",
    "      [0, 0, 1, 0 ,0 ] ( O )\n",
    "      [0, 1, 0, 1, 0 ] ( X )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "train_targets = Y_tr.reshape(-1)\n",
    "Y_tr_onehot = np.eye(nb_classes)[train_targets]\n",
    "\n",
    "test_targets = Y_te.reshape(-1)\n",
    "Y_te_onehot = np.eye(nb_classes)[test_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input size is chosen by the size of the picture. (In this case, it is 3x32x32) <br>\n",
    "The hidden size is the size of the hidden layer and it is a value we can choose arbitrarily <br>\n",
    "The output size if the number of classes we want to classify. (In this case, it is 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size_=3072\n",
    "hidden_size_=1024\n",
    "output_size_=10\n",
    "regularization_ = 0.0001\n",
    "\n",
    "network = TwoLayerNet(input_size=input_size_, hidden_size=hidden_size_, output_size=output_size_, regularization = regularization_)\n",
    "\n",
    "iters_num = 2000\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list_two = []\n",
    "train_acc_list_two = []\n",
    "test_acc_list_two = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_tr[batch_mask]\n",
    "    t_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch) \n",
    "\n",
    "    for key in ('T1', 'b1', 'T2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    for key in ('T1','T2'):\n",
    "        network.thetas[key] = network.params[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list_two.append(loss)    \n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "        test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "        train_acc_list_two.append(train_acc)\n",
    "        test_acc_list_two.append(test_acc)\n",
    "\n",
    "        print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,2), \"Test acc : \", round(test_acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plot(train_acc_list_two,test_acc_list_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ThreeLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `ThreeLayerNet` in `YourAnswer.py` file <br><br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_size=3072\n",
    "_hidden_size1=1024\n",
    "_hidden_size2=1024\n",
    "_output_size=10\n",
    "_regularization= 0.0001\n",
    "\n",
    "network = ThreeLayerNet(input_size=_input_size, hidden_size1=_hidden_size1, hidden_size2 = _hidden_size2, output_size = _output_size, regularization = _regularization)\n",
    "\n",
    "iters_num = 2000\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list_three = []\n",
    "train_acc_list_three = []\n",
    "test_acc_list_three = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_tr[batch_mask]\n",
    "    t_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('T1', 'b1', 'T2', 'b2', 'T3', 'b3'):\n",
    "\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    for key in ('T1','T2'):\n",
    "\n",
    "        network.thetas[key] = network.params[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list_three.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "        test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "        train_acc_list_three.append(train_acc)\n",
    "        test_acc_list_three.append(test_acc)\n",
    "\n",
    "        print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,2), \"Test acc : \", round(test_acc,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plot(train_acc_list_three,test_acc_list_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Selecting hyperparameters\n",
    "- Apply various hyperparameters to the TwoLayerNN and check the results\n",
    "- Find the hyperparameters that derive the best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_arr= [512, 1024, 2048]\n",
    "regularization_arr = [0.1, 0.01, 0.001]\n",
    "\n",
    "learning_rate = 0.1\n",
    "iters_num = 2000\n",
    "_input_size= 3072\n",
    "_output_size= 10\n",
    "\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "best_result = np.zeros((3,))\n",
    "\n",
    "for _hidden_size in (hidden_size_arr):\n",
    "    for _regularization in (regularization_arr):\n",
    "        \n",
    "        print (\"\\nHidden size : \" +str(_hidden_size) + \"  Regularization : \" + str(_regularization)+\"\\n\")\n",
    "\n",
    "        network = TwoLayerNet(input_size=_input_size, hidden_size=_hidden_size, output_size=_output_size, regularization = _regularization)\n",
    "\n",
    "        train_size = X_tr.shape[0]\n",
    "        batch_size = 100\n",
    "\n",
    "        iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "        for i in range(iters_num):\n",
    "\n",
    "            batch_mask = np.random.choice(train_size, batch_size)\n",
    "            x_batch = X_tr[batch_mask]\n",
    "            t_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "            grad = network.gradient(x_batch, t_batch) \n",
    "            \n",
    "            for key in ('T1', 'b1', 'T2', 'b2'):\n",
    "\n",
    "                network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "            for key in ('T1','T2'):\n",
    "\n",
    "                network.thetas[key] = network.params[key]\n",
    "\n",
    "            loss = network.loss(x_batch, t_batch)\n",
    "\n",
    "            if i % iter_per_epoch == 0:\n",
    "                train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "                test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "                print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,3), \"Test acc : \", round(test_acc,3))\n",
    "                if test_acc > best_result[0]:\n",
    "                    best_result[0] = test_acc\n",
    "                    best_result[1] = _hidden_size\n",
    "                    best_result[2] = _regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best test accruacy : ', best_result[0])\n",
    "print ('Best hyperparameter : \\n Hidden size : ' + str(best_result[1]) + '\\n Regularization : ' + str(best_result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
