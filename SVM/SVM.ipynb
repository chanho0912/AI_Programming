{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "\n",
    "#from autograd import grad\n",
    "import scipy.optimize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from YourAnswer import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Content\n",
    "\n",
    "We generate synthetic data ${\\textbf{x}^{(i)}, y^{(i)}}_{i=1,...,n}$ for binary classification that the distribution of each class is given by:\n",
    "$$\n",
    "p(\\mathbf{x}|y=1) \\sim ~ N(\\mathbf{\\mu_1}, \\mathbf{I})\\\\\n",
    "p(\\mathbf{x}|y=-1) \\sim ~ N(\\mathbf{\\mu_2}, \\mathbf{I})\\\\\n",
    "where\\ \\mu_1 = [1 \\ \\   1]^T, \\mu_2 = [-1 -1]^T, \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "n_dim = 2\n",
    "x_train, y_train = make_blobs(n_samples=100, n_features=n_dim, centers=[[1,1],[-1,-1]], shuffle=True, random_state=7)\n",
    "x_test, y_test = make_blobs(n_samples=100, n_features=n_dim, centers=[[1,1],[-1,-1]], shuffle=True, random_state=777)\n",
    "y_train = (y_train * 2) - 1\n",
    "y_test = (y_test * 2) - 1\n",
    "\n",
    "vis_data(x_train, y_train, 'r', 'Train data')\n",
    "vis_data(x_test, y_test,'r', 'Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary classification with Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function of SVM is defined as:\n",
    "$$\n",
    "\\underset{\\pmb{\\theta}_1\\in\\mathbb{R}^d, \\theta_0\\in\\mathbb{R}}{\\operatorname{minimize}} \\hspace{0.2cm} \\frac{1}{2}||\\pmb{\\theta}_1||_2^2 + C\\sum_{i=1}^nL_{Hinge}(h,(\\mathbf{x}^{(i)},y^{(i)})) \\\\\n",
    "\\text{where } L_{Hinge}(h,(\\mathbf{x},y)) = (1-y\\cdot(\\theta_1^{\\top}\\mathbf{x}+\\theta_0))_+ = (1 - s_{\\pmb{\\theta}}(y,\\mathbf{x}))_+ \n",
    "$$\n",
    "\n",
    "You will implement all parts of the function step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Score function\n",
    "##### To do : Implement `score_function` in `YourAnswer.py` <br>\n",
    "Score function:\n",
    "$$\n",
    "s_{\\pmb{\\theta}}(y, \\mathbf{x}) = y\\cdot(\\pmb{\\theta}_1^{\\top}\\mathbf{x}+\\theta_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your score function would return following values <br>\n",
    "`array([-0.88307155, -0.89749892,  0.87472908, -0.88987632, -0.89341663])` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for sanity check\n",
    "n = 5\n",
    "d = 2\n",
    "C = 1\n",
    "np.random.seed(1234)\n",
    "x_val = np.random.randn(n,d-1)\n",
    "y_val = np.random.randint(0, 2, n)\n",
    "y_val[y_val==0] = -1\n",
    "theta_val = np.random.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function(x_val, y_val, theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prediction function\n",
    "##### To do : Implement `prediction_function` in `YourAnswer.py` <br>\n",
    "\n",
    "Predict function:\n",
    "$$\n",
    "sign(\\pmb{\\theta}_1^{\\top}\\mathbf{x}+\\theta_0)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your prediction function would return following values\n",
    "\n",
    "`array([1., 1., 1., 1., 1.])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_function(x_val, theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Hinge loss\n",
    "##### To do : Implement `hinge_loss` in `YourAnswer.py` <br>\n",
    "\n",
    "\n",
    "Hinge loss:\n",
    "$$\n",
    "L_{Hinge}(h,(\\mathbf{x},y)) = (1-s_{\\pmb{\\theta}}(y, \\mathbf{x}))_+\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your hinge_loss would return following values\n",
    "\n",
    "`array([1.88307155, 1.89749892, 0.12527092, 1.88987632, 1.89341663])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hinge_loss(x_val, y_val, theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Objective function\n",
    "##### To do : Implement `objective_function` in `YourAnswer.py` <br>\n",
    "\n",
    "Objective function:\n",
    "$$\n",
    "\\text{minimize}_{\\pmb{\\theta}_1\\in\\mathbb{R}^d, \\theta_0\\in\\mathbb{R}} \\frac{1}{2}||\\pmb{\\theta}_1||_2^2 + C\\sum_{i=1}^nL_{Hinge}(h,(\\mathbf{x}^{(i)},y^{(i)})) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your objective_function would return following values\n",
    "\n",
    "`7.689171997714208`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function(theta_val, x_val, y_val, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To do : Implement `update_svm` in `YourAnswer.py` <br>\n",
    "In this part, you should compute the graident of objective function and update theta by batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize theta\n",
    "theta_init = 1e-4 * npr.randn(n_dim+1); theta_init[-1] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 5e-6\n",
    "num_iters = 2500\n",
    "C=1\n",
    "theta_opt = update_svm(theta_init, x_train, y_train, C, num_iters, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s see the obtained decision boundary and marginal hyperplanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(prediction_function, x_train, y_train, plot_support_vector=True, plot_mis=True, sklearn=False, score_func=score_function, w=theta_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(prediction_function, x_test, y_test, plot_mis=True, sklearn=False, score_func=score_function ,w=theta_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression VS SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data using logistic regression first.<br>\n",
    "We exploit the scikit learn model simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(C= 1.0, solver='lbfgs')\n",
    "logistic.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare results from two different methods for the given data\n",
    "\n",
    "Because of different optimizing ways, the decision boundaries would not be exactly the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(theta_opt, logistic, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting a dataset with multimodal distribution \n",
    "\n",
    "In section 5, we predict a dataset with multimodal distribution by using `kernel method` in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = gen_multimodal()\n",
    "x_test, y_test = gen_multimodal()\n",
    "\n",
    "vis_data(x_train, y_train, c='r')\n",
    "vis_data(x_test, y_test, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Kernel SVM\n",
    "##### To do : Implement `gaussian_kernel` and `polynomial_kernel`  in `YourAnswer.py` <br>\n",
    "Implement Gaussian kernel and polynomial kernel. <br>\n",
    "This process may take a few minutes.\n",
    "\n",
    "1. Gaussian Kernel\n",
    "$$\n",
    "K(\\pmb{x},\\pmb{z}) = \\text{exp}  (-\\frac{||\\pmb{x}-\\pmb{z}||^2 }{2\\sigma^2}) \\\\\n",
    "$$\n",
    "\n",
    "2. Polynomial Kernel\n",
    "$$\n",
    "K(\\pmb{x},\\pmb{z}) = (\\pmb{x}^T\\pmb{z}+ c)^m \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian kernel \n",
    "$\\sigma=1$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sigma=1\n",
    "svm=SVC(C=1.0, kernel= gaussian_proxy_kernel(sigma=_sigma))\n",
    "svm.fit(x_train, y_train)\n",
    "y_predict=svm.predict(x_test)\n",
    "plot_svc(svm, x_test, y_test, h=0.1, plot_support_vector=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial kernel \n",
    "m (degree)=3 and   c (bias)=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_degree=3\n",
    "_bias=1\n",
    "\n",
    "svm=SVC(C=1.0, kernel=poly_proxy_kernel(degree=_degree,bias=_bias))\n",
    "svm.fit(x_train, y_train)\n",
    "y_predict=svm.predict(x_test)\n",
    "plot_svc(svm, x_test, y_test, h=0.1, plot_support_vector=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check your implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian kernel <br>\n",
    "![nn](./gaussian_figure.png) <br>\n",
    "Poly kernel <br>\n",
    "![nn](./poly_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best parameters by grid search. \n",
    "##### To do : Implement `gridsearch`  in `YourAnswer.py` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for cross-validation\n",
    "# You can change parameters to find optimal parameter(C, gamma)\n",
    "# and also modify another parameter in GridSearchCV(CV).\n",
    "\n",
    "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],\n",
    "                     'gamma': [0.5, 1,2,3,4]}]\n",
    "\n",
    "clf = gridsearch(tuned_parameters, x_train, y_train)\n",
    "\n",
    "print(\"Grid scores\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of the best model\n",
    "clf.best_estimator_.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparson of separting hyperplane, we will train the linear-SVM by the fuction you implemented in the multimodal dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5\n",
    "num_iters = 1000\n",
    "C=1\n",
    "theta_opt = update_svm(theta_init, x_train, y_train, C, num_iters, alpha,print_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the separating hyperplane of Kernel-SVM to that of Linear-SVM in the multimodal dataset.<br>\n",
    "You can see that the kernel SVM can make a nonlinear decision boundary. Thus, when the dataset is hard to separate  <br>\n",
    "linearly, e.g., the multimodal dataset, Kernel-SVM is a more reasonable choice than the Linear-SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc_with(clf.best_estimator_, x_test, y_test, plot_support_vector=False,w =theta_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
